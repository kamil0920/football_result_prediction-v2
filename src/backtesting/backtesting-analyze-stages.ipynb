{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-12T14:59:17.394690Z",
     "start_time": "2025-02-12T14:59:16.937872Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T14:59:17.420237Z",
     "start_time": "2025-02-12T14:59:17.402337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_matches = pd.read_csv('../../data/preprocessed/preprocessed_1.csv')\n",
    "df_matches.sort_values(by=[\"season\", \"stage\", \"date\"], inplace=True)\n",
    "\n",
    "season_dummies = pd.get_dummies(df_matches['season'], prefix='season', drop_first=True)\n",
    "df_matches = df_matches.join(season_dummies)"
   ],
   "id": "1257e5966fd49d8e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T14:59:17.974154Z",
     "start_time": "2025-02-12T14:59:17.486797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "def train_and_evaluate(train_df, val_df, test_df, season, stage):\n",
    "    feature_cols = [col for col in train_df.columns if col not in\n",
    "                    [\"match_api_id\", \"result_match\", \"season\", \"stage\", \"date\", \"home_team\", \"away_team\"]]\n",
    "\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[\"result_match\"]\n",
    "\n",
    "    X_val = val_df[feature_cols]\n",
    "    y_val = val_df[\"result_match\"]\n",
    "\n",
    "    X_test = test_df[feature_cols]\n",
    "    y_test = test_df[\"result_match\"]\n",
    "\n",
    "    params = {'colsample_bytree': 0.8829682348067726,\n",
    "              'gamma': 2.153140019195803,\n",
    "              'learning_rate': 0.2839181641252695,\n",
    "              'max_depth': 10,\n",
    "              'n_estimators': 750,\n",
    "              'reg_alpha': 0.05456053939633371,\n",
    "              'reg_lambda': 0.014211434927705319,\n",
    "              'subsample': 0.8413541436147373}\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        **params,\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric=\"logloss\",\n",
    "\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], verbose=False)\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = precision_score(y_test, y_pred, average=\"binary\", zero_division=0)\n",
    "\n",
    "    return f1, season, stage, y_test.tolist(), y_pred.tolist()"
   ],
   "id": "95ca46dfdb3a69fa",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T15:00:35.810920Z",
     "start_time": "2025-02-12T14:59:17.990574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ---------------------\n",
    "# Backtesting over multiple seasons\n",
    "# ---------------------\n",
    "\n",
    "seasons = sorted(df_matches[\"season\"].unique(), reverse=True)\n",
    "backtest_results = []\n",
    "\n",
    "for i in range(1, len(seasons)):\n",
    "    target_season = seasons[0]\n",
    "    previous_seasons = seasons[1:1 + i]\n",
    "\n",
    "    target_season_df = df_matches[df_matches[\"season\"] == target_season]\n",
    "\n",
    "    target_stages = sorted(target_season_df[\"stage\"].unique())\n",
    "\n",
    "    for stage in target_stages:\n",
    "        if stage <= min(target_stages) + 1:\n",
    "            continue\n",
    "\n",
    "        train_df_prev = df_matches[df_matches[\"season\"].isin(previous_seasons)]\n",
    "        train_df_target = target_season_df[target_season_df[\"stage\"] < (stage - 1)]\n",
    "        train_df = pd.concat([train_df_prev, train_df_target], ignore_index=True)\n",
    "\n",
    "        val_df = target_season_df[target_season_df[\"stage\"] == (stage - 1)]\n",
    "        test_df = target_season_df[target_season_df[\"stage\"] == stage]\n",
    "\n",
    "        if train_df.empty or val_df.empty or test_df.empty:\n",
    "            continue\n",
    "\n",
    "        precision, season, stage, y_test, y_pred = train_and_evaluate(train_df, val_df, test_df, target_season, stage)\n",
    "\n",
    "        backtest_results.append({\n",
    "            \"season\": f\"{season} - {len(previous_seasons)}\",\n",
    "            \"stage\": stage,\n",
    "            \"train_size\": len(train_df),\n",
    "            \"precision_score\": precision,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(backtest_results)\n",
    "print(\"Backtesting results for each season and stage:\")"
   ],
   "id": "5c41cb6cbe5e2899",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backtesting results for each season and stage:\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T15:00:35.849043Z",
     "start_time": "2025-02-12T15:00:35.847162Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "#\n",
    "# # Filter worst stages\n",
    "# worst_stages = results_df.sort_values(by=\"f1_score\", ascending=True).head(5)\n",
    "#\n",
    "# for _, row in worst_stages.iterrows():\n",
    "#     y_test = row[\"y_test\"]\n",
    "#     y_pred = row[\"y_pred\"]\n",
    "#\n",
    "#     cm = confusion_matrix(y_test, y_pred)\n",
    "#\n",
    "#     plt.figure(figsize=(5, 4))\n",
    "#     sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Home not win\", \"Home win\"], yticklabels=[\"Home not win\", \"Home win\"])\n",
    "#     plt.xlabel(\"Predicted\")\n",
    "#     plt.ylabel(\"Actual\")\n",
    "#     plt.title(f\"Confusion Matrix - {row['season']} Stage {row['stage']}\")\n",
    "#     plt.show()"
   ],
   "id": "ba4fcfcfdc3f8922",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T15:15:23.779092Z",
     "start_time": "2025-02-12T15:15:23.754326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def train_and_analyze_shap_for_stage(df_matches, worst_stage_row):\n",
    "    season_info = worst_stage_row[\"season\"]\n",
    "    stage = worst_stage_row[\"stage\"]\n",
    "\n",
    "    # Extract number of previous seasons used for training\n",
    "    season, prev_seasons_count = season_info.split(\" - \")\n",
    "    prev_seasons_count = int(prev_seasons_count)\n",
    "\n",
    "    # Define training and testing data based on previous training logic\n",
    "    target_season_df = df_matches[df_matches[\"season\"] == season]\n",
    "    previous_seasons = sorted(df_matches[\"season\"].unique(), reverse=True)[1:1 + prev_seasons_count]\n",
    "\n",
    "    train_df_prev = df_matches[df_matches[\"season\"].isin(previous_seasons)]\n",
    "    train_df_target = target_season_df[target_season_df[\"stage\"] < (stage - 1)]\n",
    "    train_df = pd.concat([train_df_prev, train_df_target], ignore_index=True)\n",
    "\n",
    "    val_df = target_season_df[target_season_df[\"stage\"] == (stage - 1)]\n",
    "    test_df = target_season_df[target_season_df[\"stage\"] == stage]\n",
    "\n",
    "    if train_df.empty or val_df.empty or test_df.empty:\n",
    "        print(f\"Skipping {season} - Stage {stage} due to empty dataset.\")\n",
    "        return None\n",
    "\n",
    "    feature_cols = [col for col in train_df.columns if col not in [\"match_api_id\", \"result_match\", \"season\", \"stage\", \"date\", \"home_team\", \"away_team\"]]\n",
    "\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[\"result_match\"]\n",
    "\n",
    "    X_val = val_df[feature_cols]\n",
    "    y_val = val_df[\"result_match\"]\n",
    "\n",
    "    X_test = test_df[feature_cols]\n",
    "    y_test = test_df[\"result_match\"]\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        colsample_bytree=0.8829682348067726,\n",
    "        gamma=2.153140019195803,\n",
    "        learning_rate=0.2839181641252695,\n",
    "        max_depth=10,\n",
    "        n_estimators=750,\n",
    "        reg_alpha=0.05456053939633371,\n",
    "        reg_lambda=0.014211434927705319,\n",
    "        subsample=0.8413541436147373,\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric=\"logloss\"\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], verbose=False)\n",
    "\n",
    "    season_columns = X_test.filter(like='season').columns\n",
    "    X_test = X_test.astype({col: \"int8\" for col in season_columns})\n",
    "\n",
    "    # Compute SHAP values\n",
    "    explainer = shap.Explainer(model.predict_proba, X_test)\n",
    "    shap_values = explainer(X_test)\n",
    "\n",
    "    return model, X_test, shap_values, y_test\n"
   ],
   "id": "882079d6b0676b82",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T15:15:26.819942Z",
     "start_time": "2025-02-12T15:15:26.796411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "def plot_shap_summary(shap_values, X_test, season, stage, output_dir=\"plots\"):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    season_sanitized = season.replace(\"/\", \"-\")\n",
    "\n",
    "    # Generate the SHAP summary plot without auto-display\n",
    "    shap.summary_plot(\n",
    "        shap_values,\n",
    "        X_test,\n",
    "        feature_names=np.array(X_test.columns),\n",
    "        plot_size=(15, 25),\n",
    "        show=False\n",
    "    )\n",
    "\n",
    "    # Optional: add a figure title and tweak layout\n",
    "    plt.suptitle(f\"SHAP Summary - {season} Stage {stage}\", y=1.02)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save figure to disk\n",
    "    file_path = os.path.join(output_dir, f\"shap_summary_{season_sanitized}_stage_{stage}.png\")\n",
    "    plt.savefig(file_path)\n",
    "\n",
    "    # Close the figure so it does not appear in your notebook\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_shap_comparison(shap_values, X_test, y_test, y_pred, season, stage, output_dir=\"plots\"):\n",
    "    # Create the output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\\\n",
    "\n",
    "    season_sanitized = season.replace(\"/\", \"-\")\n",
    "\n",
    "    correct_indices = np.where(y_test == y_pred)[0]\n",
    "    incorrect_indices = np.where(y_test != y_pred)[0]\n",
    "\n",
    "    # --- SHAP for correct predictions ---\n",
    "    shap.summary_plot(\n",
    "        shap_values[correct_indices],\n",
    "        X_test.iloc[correct_indices],\n",
    "        feature_names=np.array(X_test.columns),\n",
    "        plot_size=(15, 25),\n",
    "        show=False\n",
    "    )\n",
    "    plt.suptitle(f\"SHAP Values - Correct Predictions ({season} Stage {stage})\", y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"shap_correct_{season_sanitized}_stage_{stage}.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # --- SHAP for incorrect predictions ---\n",
    "    shap.summary_plot(\n",
    "        shap_values[incorrect_indices],\n",
    "        X_test.iloc[incorrect_indices],\n",
    "        feature_names=np.array(X_test.columns),\n",
    "        plot_size=(15, 25),\n",
    "        show=False\n",
    "    )\n",
    "    plt.suptitle(f\"SHAP Values - Incorrect Predictions ({season} Stage {stage})\", y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_dir, f\"shap_incorrect_{season_sanitized}_stage_{stage}.png\"))\n",
    "    plt.close()"
   ],
   "id": "459e8ccbe57f701e",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T15:03:55.256931Z",
     "start_time": "2025-02-12T15:03:55.248171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_worst_stages = results_df.loc[results_df['f1_score'] == 0.0]\n",
    "all_worst_stages.iloc[:2]"
   ],
   "id": "6f135dc7df3c7f21",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           season  stage  train_size  f1_score  \\\n",
       "0   2015/2016 - 1      3         379       0.0   \n",
       "14  2015/2016 - 1     17         516       0.0   \n",
       "\n",
       "                            y_test                          y_pred  \n",
       "0      [1, 0, 0, 0, 0, 0, 0, 0, 0]     [0, 0, 1, 0, 0, 0, 0, 0, 1]  \n",
       "14  [0, 0, 0, 0, 0, 1, 0, 0, 1, 1]  [0, 0, 0, 0, 0, 0, 1, 1, 0, 0]  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>stage</th>\n",
       "      <th>train_size</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015/2016 - 1</td>\n",
       "      <td>3</td>\n",
       "      <td>379</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2015/2016 - 1</td>\n",
       "      <td>17</td>\n",
       "      <td>516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 1, 1]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T19:51:13.538914Z",
     "start_time": "2025-02-12T19:49:56.473655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Loop over worst-performing stages and analyze SHAP values\n",
    "for _, worst_stage_row in all_worst_stages.iloc[:2].iterrows():\n",
    "    model, X_test, shap_values, y_test = train_and_analyze_shap_for_stage(df_matches, worst_stage_row)\n",
    "\n",
    "    if model is not None:\n",
    "        season = worst_stage_row[\"season\"]\n",
    "        stage = worst_stage_row[\"stage\"]\n",
    "\n",
    "        # Generate predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Plot SHAP analysis\n",
    "        plot_shap_summary(shap_values, X_test, season, stage)\n",
    "        plot_shap_comparison(shap_values, X_test, y_test, y_pred, season, stage)"
   ],
   "id": "7e6285a902fb37ad",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(35, 7))\n",
    "for season in results_df[\"season\"].unique():\n",
    "    season_results = results_df[results_df[\"season\"] == season]\n",
    "    plt.plot(season_results[\"stage\"], season_results[\"f1_score\"], marker=\"o\", label=f\"Season {season}\")\n",
    "\n",
    "plt.xlabel(\"Stage\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"Rolling Backtesting Performance Across Seasons\")\n",
    "plt.legend(title=\"Season\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "41e4f756b54a0429",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "avg_results = results_df.groupby(\"season\")[\"f1_score\"].mean().reset_index()\n",
    "avg_results.rename(columns={\"f1_score\": \"avg_f1_score\"}, inplace=True)\n",
    "print(\"\\nAverage F1 Score for each season:\")\n",
    "print(avg_results)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(avg_results[\"season\"].astype(str), avg_results[\"avg_f1_score\"], color='skyblue')\n",
    "plt.xlabel(\"Season\")\n",
    "plt.ylabel(\"Average F1 Score\")\n",
    "plt.title(\"Average F1 Score per Season\")\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ],
   "id": "7601ec8dc88fa265",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b2a3daaf3ae1ef31",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b797172af3a4d05a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
