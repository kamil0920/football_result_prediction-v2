{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_matches = pd.read_csv('../../data/preprocessed/preprocessed_1.csv')\n",
    "df_matches.sort_values(by=[\"season\", \"stage\", \"date\"], inplace=True)\n",
    "\n",
    "season_dummies = pd.get_dummies(df_matches['season'], prefix='season', drop_first=True)\n",
    "df_matches = df_matches.join(season_dummies)"
   ],
   "id": "1257e5966fd49d8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "def train_and_evaluate(train_df, val_df, test_df, season, stage):\n",
    "    feature_cols = [col for col in train_df.columns if col not in\n",
    "                    [\"match_api_id\", \"result_match\", \"season\", \"stage\", \"date\", \"home_team\", \"away_team\"]]\n",
    "\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[\"result_match\"]\n",
    "\n",
    "    X_val = val_df[feature_cols]\n",
    "    y_val = val_df[\"result_match\"]\n",
    "\n",
    "    X_test = test_df[feature_cols]\n",
    "    y_test = test_df[\"result_match\"]\n",
    "\n",
    "    params = {'colsample_bytree': 0.8829682348067726,\n",
    "              'gamma': 2.153140019195803,\n",
    "              'learning_rate': 0.2839181641252695,\n",
    "              'max_depth': 10,\n",
    "              'n_estimators': 750,\n",
    "              'reg_alpha': 0.05456053939633371,\n",
    "              'reg_lambda': 0.014211434927705319,\n",
    "              'subsample': 0.8413541436147373}\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        **params,\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric=\"logloss\",\n",
    "\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], verbose=False)\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"binary\")\n",
    "\n",
    "    return f1, season, stage, y_test.tolist(), y_pred.tolist()"
   ],
   "id": "95ca46dfdb3a69fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---------------------\n",
    "# Backtesting over multiple seasons\n",
    "# ---------------------\n",
    "\n",
    "seasons = sorted(df_matches[\"season\"].unique(), reverse=True)\n",
    "backtest_results = []\n",
    "\n",
    "for i in range(1, len(seasons)):\n",
    "    target_season = seasons[0]\n",
    "    previous_seasons = seasons[1:1 + i]\n",
    "\n",
    "    target_season_df = df_matches[df_matches[\"season\"] == target_season]\n",
    "\n",
    "    target_stages = sorted(target_season_df[\"stage\"].unique())\n",
    "\n",
    "    for stage in target_stages:\n",
    "        if stage <= min(target_stages) + 1:\n",
    "            continue\n",
    "\n",
    "        train_df_prev = df_matches[df_matches[\"season\"].isin(previous_seasons)]\n",
    "        train_df_target = target_season_df[target_season_df[\"stage\"] < (stage - 1)]\n",
    "        train_df = pd.concat([train_df_prev, train_df_target], ignore_index=True)\n",
    "\n",
    "        val_df = target_season_df[target_season_df[\"stage\"] == (stage - 1)]\n",
    "        test_df = target_season_df[target_season_df[\"stage\"] == stage]\n",
    "\n",
    "        if train_df.empty or val_df.empty or test_df.empty:\n",
    "            continue\n",
    "\n",
    "        f1, season, stage, y_test, y_pred = train_and_evaluate(train_df, val_df, test_df, target_season, stage)\n",
    "\n",
    "        backtest_results.append({\n",
    "            \"season\": f\"{season} - {len(previous_seasons)}\",\n",
    "            \"stage\": stage,\n",
    "            \"train_size\": len(train_df),\n",
    "            \"f1_score\": f1,\n",
    "            \"y_test\": y_test,\n",
    "            \"y_pred\": y_pred\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(backtest_results)\n",
    "print(\"Backtesting results for each season and stage:\")"
   ],
   "id": "5c41cb6cbe5e2899",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "#\n",
    "# # Filter worst stages\n",
    "# worst_stages = results_df.sort_values(by=\"f1_score\", ascending=True).head(5)\n",
    "#\n",
    "# for _, row in worst_stages.iterrows():\n",
    "#     y_test = row[\"y_test\"]\n",
    "#     y_pred = row[\"y_pred\"]\n",
    "#\n",
    "#     cm = confusion_matrix(y_test, y_pred)\n",
    "#\n",
    "#     plt.figure(figsize=(5, 4))\n",
    "#     sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Home not win\", \"Home win\"], yticklabels=[\"Home not win\", \"Home win\"])\n",
    "#     plt.xlabel(\"Predicted\")\n",
    "#     plt.ylabel(\"Actual\")\n",
    "#     plt.title(f\"Confusion Matrix - {row['season']} Stage {row['stage']}\")\n",
    "#     plt.show()"
   ],
   "id": "ba4fcfcfdc3f8922",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "all_worst_stages = results_df.loc[results_df['f1_score'] == 0.0]",
   "id": "2d01fdb07b5e02a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def train_and_analyze_shap_for_stage(df_matches, worst_stage_row):\n",
    "    season_info = worst_stage_row[\"season\"]\n",
    "    stage = worst_stage_row[\"stage\"]\n",
    "\n",
    "    # Extract number of previous seasons used for training\n",
    "    season, prev_seasons_count = season_info.split(\" - \")\n",
    "    prev_seasons_count = int(prev_seasons_count)\n",
    "\n",
    "    # Define training and testing data based on previous training logic\n",
    "    target_season_df = df_matches[df_matches[\"season\"] == season]\n",
    "    previous_seasons = sorted(df_matches[\"season\"].unique(), reverse=True)[1:1 + prev_seasons_count]\n",
    "\n",
    "    train_df_prev = df_matches[df_matches[\"season\"].isin(previous_seasons)]\n",
    "    train_df_target = target_season_df[target_season_df[\"stage\"] < (stage - 1)]\n",
    "    train_df = pd.concat([train_df_prev, train_df_target], ignore_index=True)\n",
    "\n",
    "    val_df = target_season_df[target_season_df[\"stage\"] == (stage - 1)]\n",
    "    test_df = target_season_df[target_season_df[\"stage\"] == stage]\n",
    "\n",
    "    if train_df.empty or val_df.empty or test_df.empty:\n",
    "        print(f\"Skipping {season} - Stage {stage} due to empty dataset.\")\n",
    "        return None\n",
    "\n",
    "    feature_cols = [col for col in train_df.columns if col not in\n",
    "                    [\"match_api_id\", \"result_match\", \"season\", \"stage\", \"date\", \"home_team\", \"away_team\"]]\n",
    "\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[\"result_match\"]\n",
    "\n",
    "    X_val = val_df[feature_cols]\n",
    "    y_val = val_df[\"result_match\"]\n",
    "\n",
    "    X_test = test_df[feature_cols]\n",
    "    y_test = test_df[\"result_match\"]\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        colsample_bytree=0.8829682348067726,\n",
    "        gamma=2.153140019195803,\n",
    "        learning_rate=0.2839181641252695,\n",
    "        max_depth=10,\n",
    "        n_estimators=750,\n",
    "        reg_alpha=0.05456053939633371,\n",
    "        reg_lambda=0.014211434927705319,\n",
    "        subsample=0.8413541436147373,\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric=\"logloss\"\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], verbose=False)\n",
    "\n",
    "    season_columns = X_test.filter(like='season').columns\n",
    "    X_test = X_test.astype({col: \"int8\" for col in season_columns})\n",
    "\n",
    "    # Compute SHAP values\n",
    "    explainer = shap.Explainer(model.predict_proba, X_test)\n",
    "    shap_values = explainer(X_test)\n",
    "\n",
    "    return model, X_test, shap_values, y_test\n"
   ],
   "id": "882079d6b0676b82",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_shap_summary(shap_values, X_test, season, stage):\n",
    "    # Use SHAPâ€™s plot_size instead of your own figure, and do not auto-show.\n",
    "    shap.summary_plot(\n",
    "        shap_values,\n",
    "        X_test,\n",
    "        feature_names=np.array(X_test.columns),\n",
    "        plot_size=(15, 25),\n",
    "        show=False\n",
    "    )\n",
    "    # Use a suptitle to position above the entire plot\n",
    "    plt.suptitle(f\"SHAP Summary - {season} Stage {stage}\", y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_shap_comparison(shap_values, X_test, y_test, y_pred, season, stage):\n",
    "    correct_indices = np.where(y_test == y_pred)[0]\n",
    "    incorrect_indices = np.where(y_test != y_pred)[0]\n",
    "\n",
    "    # SHAP for correct predictions\n",
    "    shap.summary_plot(\n",
    "        shap_values[correct_indices],\n",
    "        X_test.iloc[correct_indices],\n",
    "        feature_names=np.array(X_test.columns),\n",
    "        plot_size=(15, 25),\n",
    "        show=False\n",
    "    )\n",
    "    plt.suptitle(f\"SHAP Values - Correct Predictions ({season} Stage {stage})\", y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # SHAP for incorrect predictions\n",
    "    shap.summary_plot(\n",
    "        shap_values[incorrect_indices],\n",
    "        X_test.iloc[incorrect_indices],\n",
    "        feature_names=np.array(X_test.columns),\n",
    "        plot_size=(15, 25),\n",
    "        show=False\n",
    "    )\n",
    "    plt.suptitle(f\"SHAP Values - Incorrect Predictions ({season} Stage {stage})\", y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "459e8ccbe57f701e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Loop over worst-performing stages and analyze SHAP values\n",
    "for _, worst_stage_row in all_worst_stages.iterrows():\n",
    "    model, X_test, shap_values, y_test = train_and_analyze_shap_for_stage(df_matches, worst_stage_row)\n",
    "\n",
    "    if model is not None:\n",
    "        season = worst_stage_row[\"season\"]\n",
    "        stage = worst_stage_row[\"stage\"]\n",
    "\n",
    "        # Generate predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Plot SHAP analysis\n",
    "        plot_shap_summary(shap_values, X_test, season, stage)\n",
    "        plot_shap_comparison(shap_values, X_test, y_test, y_pred, season, stage)"
   ],
   "id": "7e6285a902fb37ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(35, 7))\n",
    "for season in results_df[\"season\"].unique():\n",
    "    season_results = results_df[results_df[\"season\"] == season]\n",
    "    plt.plot(season_results[\"stage\"], season_results[\"f1_score\"], marker=\"o\", label=f\"Season {season}\")\n",
    "\n",
    "plt.xlabel(\"Stage\")\n",
    "plt.ylabel(\"F1 Score\")\n",
    "plt.title(\"Rolling Backtesting Performance Across Seasons\")\n",
    "plt.legend(title=\"Season\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "41e4f756b54a0429",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "avg_results = results_df.groupby(\"season\")[\"f1_score\"].mean().reset_index()\n",
    "avg_results.rename(columns={\"f1_score\": \"avg_f1_score\"}, inplace=True)\n",
    "print(\"\\nAverage F1 Score for each season:\")\n",
    "print(avg_results)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(avg_results[\"season\"].astype(str), avg_results[\"avg_f1_score\"], color='skyblue')\n",
    "plt.xlabel(\"Season\")\n",
    "plt.ylabel(\"Average F1 Score\")\n",
    "plt.title(\"Average F1 Score per Season\")\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ],
   "id": "7601ec8dc88fa265",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b2a3daaf3ae1ef31",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b797172af3a4d05a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
